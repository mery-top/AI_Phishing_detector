{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1...\n",
      "Processing batch 2...\n",
      "Processing batch 3...\n",
      "Processing batch 4...\n",
      "Processing batch 5...\n",
      "Processing batch 6...\n",
      "Processing batch 7...\n",
      "Processing batch 8...\n",
      "Processing batch 9...\n",
      "Processing batch 10...\n",
      "Processing batch 11...\n",
      "Processing batch 12...\n",
      "Processing batch 13...\n",
      "Processing batch 14...\n",
      "Processing batch 15...\n",
      "Processing batch 16...\n",
      "Processing batch 17...\n",
      "Processing batch 18...\n",
      "Processing batch 19...\n",
      "Processing batch 20...\n",
      "Processing batch 21...\n",
      "Processing batch 22...\n",
      "Processing batch 23...\n",
      "Processing batch 24...\n",
      "Processing batch 25...\n",
      "Processing batch 26...\n",
      "Processing batch 27...\n",
      "Processing batch 28...\n",
      "Processing batch 29...\n",
      "Processing batch 30...\n",
      "Processing batch 31...\n",
      "Processing batch 32...\n",
      "Processing batch 33...\n",
      "Processing batch 34...\n",
      "Processing batch 35...\n",
      "Processing batch 36...\n",
      "Processing batch 37...\n",
      "Processing batch 38...\n",
      "Processing batch 39...\n",
      "Processing batch 40...\n",
      "Processing batch 41...\n",
      "Processing batch 42...\n",
      "Processing batch 43...\n",
      "Processing batch 44...\n",
      "Processing batch 45...\n",
      "Processing batch 46...\n",
      "Processing batch 47...\n",
      "Processing batch 48...\n",
      "Processing batch 49...\n",
      "Processing batch 50...\n",
      "Processing batch 51...\n",
      "Processing batch 52...\n",
      "Processing batch 53...\n",
      "Processing batch 54...\n",
      "Processing batch 55...\n",
      "Processing batch 56...\n",
      "Processing batch 57...\n",
      "Processing batch 58...\n",
      "Processing batch 59...\n",
      "Processing batch 60...\n",
      "Processing batch 61...\n",
      "Processing batch 62...\n",
      "Processing batch 63...\n",
      "Processing batch 64...\n",
      "Processing batch 65...\n",
      "Processing batch 66...\n",
      "X shape: (651191, 4816504)\n",
      "y shape: (651191,)\n",
      "X_train shape: (520952, 4816504)\n",
      "X_test shape: (130239, 4816504)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"dataset/malicious_phish.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['type'] = label_encoder.fit_transform(data['type'])\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"char_wb\", ngram_range=(3,5))\n",
    "vectorizer.fit(data['url'])  # Fit on the entire dataset\n",
    "\n",
    "batch_size = 10000\n",
    "sparse_matrices =[]\n",
    "\n",
    "\n",
    "for i in range (0, len(data), batch_size):\n",
    "    print(f\"Processing batch {i // batch_size + 1}...\")\n",
    "    batch = data['url'][i:i + batch_size]\n",
    "    sparse_matrices.append(vectorizer.transform(batch))\n",
    "\n",
    "\n",
    "X = vstack(sparse_matrices)\n",
    "\n",
    "y = data['type']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 )\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1186s\u001b[0m 9s/step - accuracy: 0.8071 - loss: 0.6382 - val_accuracy: 0.9410 - val_loss: 0.1821\n",
      "Epoch 2/5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1167s\u001b[0m 9s/step - accuracy: 0.9859 - loss: 0.0555 - val_accuracy: 0.9445 - val_loss: 0.2181\n",
      "Epoch 3/5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1411s\u001b[0m 11s/step - accuracy: 0.9974 - loss: 0.0087 - val_accuracy: 0.9455 - val_loss: 0.2347\n",
      "Epoch 4/5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1189s\u001b[0m 10s/step - accuracy: 0.9995 - loss: 0.0033 - val_accuracy: 0.9480 - val_loss: 0.2384\n",
      "Epoch 5/5\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1209s\u001b[0m 10s/step - accuracy: 0.9993 - loss: 0.0031 - val_accuracy: 0.9475 - val_loss: 0.2340\n",
      "\u001b[1m4070/4070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1640s\u001b[0m 403ms/step - accuracy: 0.9404 - loss: 0.2583\n",
      "Test Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation=\"relu\",input_dim=X_train.shape[1]),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Subset the data to only 10,000 URLs\n",
    "X_train_subset = X_train[:10000]\n",
    "y_train_subset = y_train[:10000]\n",
    "\n",
    "# Train the model using only the subset\n",
    "model.fit(X_train_subset, y_train_subset, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f * 100}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('phishing_url_model.h5')\n",
    "\n",
    "# Save the vectorizer using pickle\n",
    "import pickle\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "# Check unique values in y_train and y_test\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))\n",
    "\n",
    "# If you find labels out of range, you need to handle them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier '.2f * 100' for object of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f * 100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid format specifier '.2f * 100' for object of type 'float'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('phishing_url_model.h5')\n",
    "\n",
    "# Save the vectorizer using pickle\n",
    "import pickle\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
